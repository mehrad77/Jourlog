---
date: '2020-04-02'
title: 'Have an offline version of a website'
category: 'English Posts'
tags: ['Technical Notes']
dir: 'ltr'
banner: '/assets/2020/github-pages.jpg'
draft: false
description: "I remembered as a child there was some softwares on windows OS to do that. so we had a Dial-up connection and sometimes I would download all of a website so I won't need Internet access to browse it. but now I was on Linux and knew there must be an easier way to do this."
---

At the beginning of 2017, [Shamim](https://irandestina.github.io/website/index.html%3Fp=1269.html), [Maryam](https://irandestina.github.io/website/index.html%3Fp=1273.html), and [Parastoo](https://likeaswallow.com/) met each other at the university, when they were passing master of tourism management course. Years of friendship and traveling made them closer to each other. And one day they decided to share this wonderful feeling with others. the result was [Irandestina group](https://irandestina.github.io/website/). Irandestina was the space to experience the good filling of traveling. friendly, creative and adventurous trips with respect to the environment and the lives of host communities. later they've also pivoted to accessible tourism but with the fall of Iran economy now 2 of them migrated and Irandestina stopped here.

then around July of 2019 Parastoo asked me if I can find a way to keep Irandestina website content without the need to pay for hosting. and I said "Yes!". my plan was to download the whole website and put it over **Github/Gitlab Pages** for free and point the domain to there too.

I remembered as a child there was some softwares on windows OS to do that. so we had a Dial-up connection and sometimes I would download all of a website so I won't need Internet access to browse it. but now I was on Linux and knew there must be an easier way to do this.

[_And it is! it's called **WGET**!_](https://en.wikipedia.org/wiki/Wget)

```bash
$ wget \
     --recursive \
     --no-clobber \
     --page-requisites \
     --html-extension \
     --convert-links \
     --domains irandestina.com \
     --no-parent \
         www.irandestina.com
```

all you I had to do was this.

The options are:

- **--recursive:** the most important flag, download sub URLs in given address.
- **--domains irandestina.com:** don't follow links outside irandestina.com.
- **--no-parent:** don't follow the links outside of provided URL, can b used to download only one sub URL.
- **--page-requisites:** get all the elements (images, CSS, JS and others) of the page.
- **--html-extension:** save files with the .html extension.
- **--convert-links:** convert HTML links to local version so site is navigable offline.
- **--no-clobber:** don't overwrite the already downloaded files (used when the download is interrupted and resumed).

for more options you can check out [it's manual page](https://www.gnu.org/software/wget/manual/html_node/Recursive-Retrieval-Options.html). also [read more at thegeekstuff.com](https://www.thegeekstuff.com/2009/09/the-ultimate-wget-download-guide-with-15-awesome-examples/)

so now let's ship this static website we just downloaded. I hope you know how to work with Git. create a new repository, go inside the downloaded directory and do this:

1. `git init`
2. `git add *`
3. `git commit -m "add static content"`
4. `git remote add origin git@github.com:m[REPOSITORY ADDRESS]`
5. `git push -u origin master`

great! now open the repository setting and make the `master` branch the Source for Github Pages. you can also add a `CNAME` file to the repo so you can have your custom domain. [read more at Github Help.](https://help.github.com/en/github/working-with-github-pages/configuring-a-custom-domain-for-your-github-pages-site)

![image-20200402172636792](/assets/2020/github-pages.jpg)
